"""CapionMe-Flickr8k-5c.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Zt7M-iLyqmRzDtNNymx_JnuTqzG_Fvs

# CaptionMe Project


---


We are working on a project which has the goal to generate some categories about a given image and  reciprocally.
To achieve this project we have listed the following steps :

## 1. Download the dataset of Flickr30K from Kaggle website

 * Download the datset using this URL :
 https://www.kaggle.com/hsankesara/flickr-image-dataset
 * Move it to your Drive 
 * Extract the data - unzip the folder - even if it takes a looooong time
 * Create a shortcut of `archive` into you `/content/drive/MyDrive`

---

Note:

Flickr30K contains 31783 images  with 5 different comments  corresponding to them. The idea there is to take a part of the dataset at the first 8000 images in total for those reasons :
* The memory capacity of Disk in Google Drive, Google Colab : we tested the way with the whole 31783 images dataset
   and taking only 1 comment per image, that is to say a total of 31783 comments.
   The result was very bad, around O.38 %  of accurray even trying to tune the model.
* The model of text generation should be trained on various comments per image to be more accurate and in order to reduce the loss and val loss.

"""


"""# 2 - Dataset Management"""

# Paths to the dataset
#Main data/model directory
FLICKR_SEED_PATH_DIR = "../flickr30k_images/"
#Comments csv
FLICKR_PATH_CSV = FLICKR_SEED_PATH_DIR + "results.csv"
#Image dir
FLICKR_PATH_DIR = FLICKR_SEED_PATH_DIR + "flickr30k_images/"
FLICKR_PATH_DIR_8K_5C = FLICKR_SEED_PATH_DIR + "flickr8K_5C/"
FLICKR_PATH_DIR_8K_5C_MODEL = FLICKR_PATH_DIR_8K_5C + "model_text_generation/"
FLICKR_PATH_DIR_8K_5C_MODEL_CHECKPOINTS_DIR = FLICKR_PATH_DIR_8K_5C_MODEL + "model_generation_sentences/"

"""# Pre-processing Data

Way to proceed : 
There are to ways to proceed in this project :
   * The first considers 5 comments per image but we are not going to use this way because of the lack of memory :
   * All the images in the dataset has both <b>5 different comments  and  image in the folder
   * The `flickr_csv` is a Dataframe containing the flickr30k dataset and other columns , the whole sorted by `image_name` column. 
* The second considers 1 comment per image and we are going to use this way fisrt !

We will thus condiser `8000 images and 8000 * 5 = 40000 coments` 
"""

# Checkout the labels of our data
import pandas as pd
import os

flickr_csv = pd.read_csv(FLICKR_PATH_CSV, error_bad_lines=False, sep="|")
 
# We notice that there are spaces in the features names : let's removing them
# Columns : 'image_name', 'comment_number', 'comment'
flickr_csv.columns = [col.strip() for col in flickr_csv.columns]

# We notice that there are spaces in the leading of cells : let's removing them
for col in flickr_csv.columns:
  flickr_csv[col] = flickr_csv[col].apply(lambda x: str(x).lstrip())

# Put all the comments into lowercase
flickr_csv['comment'] = flickr_csv['comment'].apply(lambda x: x.lower())


# Add new column 'image_name_process' by concatenating 'image_name' without the extension '.jpg' and the 'comment_number'
flickr_csv["image_name_process"] = flickr_csv.apply(lambda row: row["image_name"].split(".")[0] + "." + row["comment_number"], axis=1)

# Sort flickr_csv by 'image_name_process'
flickr_csv = flickr_csv.sort_values("image_name_process")

# Keep only 8090 lines
flickr_csv = flickr_csv.loc[:8000*5-1]

flickr_csv.head()

"""Verifying if all the `image_name` in the `Dataframe flickr_csv` have an image in the folder `FLICKR_PATH_DIR`."""

# Verfify if an image_name corresponds to an existing image
image_name_in_folder = os.listdir(FLICKR_PATH_DIR)

cpt_index_images_name_to_drop = 0

for index, row in flickr_csv.iterrows():
  if row['image_name'] not in image_name_in_folder:
    cpt_index_images_name_to_drop += 1

if cpt_index_images_name_to_drop == 0:
  print("All image_name values in the dataset has an image in the folder !")
else:
  print("Some image_name values in the dataset hasn't an image in the folder !")

"""Verifying if all the `image_name` in the `Dataframe flickr_csv` have 5 comments."""

# Verify how many comments are there for every image
flickr_gbcount = flickr_csv.groupby("image_name").count()

# Count the number of 'image_name_process' after groupby and count inferior to 5
cpt = 0

for index, row in flickr_gbcount.iterrows():
  if row["image_name_process"] == 5:
    cpt += 1

if len(flickr_gbcount) == cpt:
  print("All the images have 5 comments !")
else:
  print("Some images have less that 5 comments !\nThis must be due to the reduction number to 1 of comments per image...")

flickr_gbcount.head()

"""## Remove punctuation

Punctuation can provide grammatical context to a sentence which supports our understanding. But for our vectorizer which counts the number of words and not the context, it does not add value, so we remove all special characters. eg: How are you?->How are you
"""

import string

# Function to remove the punctuation
def remove_punctuation(text):
  """
  Takes a text and remove its punctuation.
  """
  text_without_punct = "".join([char for char in text if char not in string.punctuation])

  return text_without_punct

flickr_csv["comment_text_clean"] = flickr_csv["comment"].apply(lambda x: remove_punctuation(str(x)))

flickr_csv.head()

"""## Tokenization

Tokenizing separates text into units such as sentences or words. It gives structure to previously unstructured text. eg: Plata o Plomo-> ‘Plata’,’o’,’Plomo’.
"""

import re

# Function to Teokenize words
def tokenize(text):
  """
  Takes a text and tokenize it.
  """
  # W+ means that either a word character (A-Za-z0-9_) or a dash (-) is accepted
  tokens = re.split("\W+", text) 

  return tokens

flickr_csv["comment_text_tokenized"] = flickr_csv["comment_text_clean"].apply(lambda x: tokenize(x))

flickr_csv.head()

"""# 3 - End to End caption generator """

from keras.applications.vgg16 import VGG16
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input

from IPython.display import Image

# Display one image
Image('/content/drive/MyDrive/L3 DANT/Data-Science/archive/flickr30k_images/flickr30k_images/1000092795.jpg')

"""## Data preprocessing

 1 - Transform `comment` column into lowercase</br>
 2 - Remove punctuation of `comment` column --> store into `comment_text_clean` column</br>
 3 - Tokenize `comment_text_clean` column --> store into `comment_text_tokenized`</br>
 4 - Add `"startseq"` and `"endseq"` respetively at the begining and at the end of the comment for each row of `comment_text_clean`. This way will <b>allow the model to detect when the sentence begins and when it ends</b> --> store into `surrounded` column

 You have certainly noticed that we are not using  `comment_text_tokenized`, this is because we will probably test different ways to perform all the steps to improve our execution scores.
"""

# Add new column 'comment_text_surrounded' by concatenating 'startseq', 'comment_text_tokenized' and 'endseq' together
flickr_csv["comment_text_surrounded"] = flickr_csv["comment_text_tokenized"].apply(lambda x : 'startseq ' + ' '.join(x) + ' endseq')
flickr_csv.head()

flickr_csv.describe

# List of all caption names
caption_images_list = []

# We make a verification of unique image even if we are using 1 or 5 comments
image_index = list(map(lambda x: x.split(".")[0] , flickr_csv["image_name"].unique().tolist()))

print("Number of unique image_name : " + str(len(image_index)))

"""To split efficiently our dataset of <b>8000 samples</b>, we will use the commonly used <b>ratio 70-15-15</b>:
* 70% ≃ 5600</b> --> Training Set
* 15% ≃ 1200</b> --> Validation Set
* 15% ≃ 1200</b> --> Testing Set
"""

# Split the list into train validate (5600 + 1200) and test (1200) samples 
train_validate_images = image_index[:5600 + 1200]
test_images = image_index[5600 + 1200:]
  
  
print("Number of training samples/images : " + str(len(train_validate_images)))
print("Number of testing samples/images  : " + str(len(test_images)))
print("Total                             : " + str(len(train_validate_images) + len(test_images)))

"""The following cell will make feature predictions on training images using pre trained VGG16 model. The prediction will be saved in a `.pkl` binary file called `train_validate_feature_[date & time].pkl`.

This backup of the model allows us to use the predictions by loading the file and thus avoid to rerun the cell and waist al lot of time. 

Note :
* The `if False:` is there just in case someone run this cell without reading the doc.    
* Running this cell on <b>158815 samples</b> took <b>19516 sec ≃ 05:24:00 hours</b>. Since there are only unique 31783 samples in the dataset, it was useless and a waist of time.
* Runing this cell on <b>27016 samples </b>took <b>8098 sec ≃ 02:15:00 hours</b>.

Whether we use 1 or 5 comments, the splitting stay the same.

"""

# +------------------------------------------------------------------------------------------------------------+
# |                 DO NOT RUN THIS CELL IF THE MODEL HAS ALREADY BEEN SAVED IN A '.pkl' FILE                  |
# +------------------------------------------------------------------------------------------------------------+

from pickle import dump
from datetime import datetime
import time

# extract features from each photo in the directory
def extract_features(directory, image_keys):
    # load the model
    model = VGG16()
    
    # re-structure the model
    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)
    
    # summarize
    print(model.summary())
      
    # extract features from each photo
    features = dict()

    # counter
    counter = 0
    
    for name in image_keys:
        
        # load an image from file
        filename = directory + name + '.jpg'
        
        # load the image and convert it into target size of 224*224
        image = load_img(filename, target_size=(224, 224))
        
        # convert the image pixels to a numpy array
        image = img_to_array(image)
        
        # reshape data for the model
        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
        
        # prepare the image for the VGG model
        image = preprocess_input(image)
        
        # get features
        feature = model.predict(image, verbose=0)
        
        # get image id
        image_id = name.split('.')[0]
        
        # store feature
        features[image_id] = feature
        
        counter += 1
        print("Iter nb : " + str(counter))
        

    return features


start_time = time.time()

# Extracting image features for train_validate_images
train_validate_features = extract_features(FLICKR_PATH_DIR, train_validate_images)

print("--- %s seconds ---" % (time.time() - start_time))

now = datetime.now()

# dd-mm-YY_H-M-S
dt_string = now.strftime("%d-%m-%Y_%H-%M-%S")
filename = FLICKR_PATH_DIR_8K_5C + 'train_validate_features_' + dt_string + '.pkl'

# Save the model .pkl file
dump(train_validate_features, open(filename, 'wb'))

"""<u>Note</u> :
` train_validate_features` as the following format:
```python
  ''' 
  Contains the features extraction of VGG16 for a given image_name
  '''

  dict(string : list(flaot32)) train_validate_feature
```
"""

import pickle

# Load the Model back from file
dt_string = "28-05-2021_13-21-58"
fn_pkl_8K_5C = FLICKR_PATH_DIR_8K_5C + 'train_validate_features_' + dt_string + '.pkl' 

with open(fn_pkl_8K_5C, 'rb') as file:  
    # 
    train_validate_features = pickle.load(file)

"""## Preparating data for the machine learning model that make sentences"""

# load libraries
import numpy as np
from keras.models import Model, load_model
from keras.layers import Input, Dense, Dropout, LSTM, Embedding
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical, plot_model
from keras.layers.merge import add
from keras.callbacks import ModelCheckpoint

print("Number of training and validation samples : " + str(len(train_validate_features)))

print("{} : {}".format(list(train_validate_features.keys())[0], train_validate_features[list(train_validate_features.keys())[0]] ))

"""THIS IS A IDEAL CASE USING THE WHOLE DATASET OF 158915 SAMPLES!</b></br>
As said above, every caption have 5 comments so we will be able to train and validate our sentence predictions model over `5 * len(train_validate_features) = 5 * 6800 = ` <b>`34000`</b>.</br>

THIS IS A POSSIBLE CASE NOT USING THE WHOLE DATASET BUT ONLY 31783 SAMPLES!</b></br>
As said above, every caption have <b>5 comments but our memory capacity is limited</b> to the storage available on a Googel Drive account. Therefore, we are only going to deal with <b>1 comment per image</b>, the one with a `image_name_process` ending by `.0`.</br>
To split efficiently our dataset of <b>31783 samples</b>, we will use the commonly used <b>ratio 70-15-15</b>:
70% ≃ 22249</b> --> Training Set
15% ≃ 4767</b> --> Validation Set
15% ≃ 4767</b> --> Testing Set

"""

# Make a dictionary : { 'image_name_process' : 'comment_text_surrounded'}
train_validate_image_caption = {}

list_keys_train_validate_features = list(train_validate_features.keys())

for index, row in flickr_csv.iterrows():
    """
    """

    # Check whether the image is available in both train_validate_images list and train_validate_features dictionary
    image_name = row["image_name"].split('.')[0]

    if (image_name in train_validate_images) and (image_name in list_keys_train_validate_features):
        
        train_validate_image_caption.update({row["image_name_process"] : row["comment_text_surrounded"]})

print(len(train_validate_image_caption))

if (len(train_validate_image_caption) == len(train_validate_images) * 5 
    and len(train_validate_image_caption) == len(list_keys_train_validate_features) * 5): 
    print("Training-Validation set has 5 comments for each training-validation images and key validation feature ! \
          \ne.g We have a set of 5 comments per training-validation captions and features !")
else:
  print("Training-Validation set has not 5 comments for each training-validation images and key validation feature ! \
          \ne.g We haven't got a set of 5 comments per training-validation captions and features.\
          \nIf it's not an error and you're juste using 1 comment -> OK\
          \nElse -> FIX IT BEFORE CONTINUING !")

"""Note :
* Since keys of `train_validate_image_caption` dictionary are `image_name_process` and thus has the following format `IMAGE_NAME.COMMENT_NUMBER` (ex : `1000092795.0`), it's needed to <b>extract the `image_name` without the `comment_number` for some processes</b>.

* `train_validate_image_caption.values()` contains noting else than the `flickr_csv`column `comment_text_surronded`
"""

print(list(train_validate_image_caption.values())[0])
print(list(train_validate_image_caption.keys())[0])

Image(FLICKR_PATH_DIR + list(train_validate_image_caption.keys())[0].split(".")[0] + '.jpg')

# Initialize tokenizer
tokenizer = Tokenizer()

# Create word count dictionary on captions list
tokenizer.fit_on_texts(list(train_validate_image_caption.values()))

# How many words are there in the vocabulary?
# store the total length in vocab_len and add 1 because word_index starts with 1 not 0
vocab_len = len(tokenizer.word_index) + 1

# Iterate over train_validate_image_cation values
# Split each value and take the length
# Store the length of the maximum sentence
max_len = max(len(train_validate_image_caption[image].split()) for image in train_validate_image_caption)

print(train_validate_images)

# Get all rows (e.g all captions) which has train_validate_images values
# Thus we have to take all the rows whose its image_name has been trained previously
train_validate_images_processed = [row["image_name_process"] for index, row in flickr_csv.iterrows() if row["image_name"].split(".")[0] in train_validate_images]

len(train_validate_images_processed)

"""<u>Note</u>:
* As said earlier, we split our initial <b>8000 * 5 = 40000 samples</b> dataset following the commonly used <b>ratio 70-15-15</b>:
  * 70% ≃ 28000 --> Training Set  --> `BATCH_SIZE = 1750`
  * 15% ≃ 6000 --> Validation Set --> `BATCH_SIZE = 375`
  * 15% ≃ 6000 --> Testing Set 
* <strike>Since now we are not dealing with unique images ans thus `image_name` we will apply a factor of 5, corresponding to the number of comments available for each image. Moreover, don't forget that the Training and Validation part should represent 135080 samples. Thus, spliting our 158915 data using same way, gives us those results:
  * 70% ≃ 111243 --> Training Set
  * 15% ≃ 23837 --> Validation Set --> `BATCH_SIZE = 33`
  * 15% ≃ 23837 --> Testing Set --> `BATCH_SIZE = 11`</strike>
* To observe the time taken by the construction of each training and validation text set, let's print the timers.

Note :
* Each caption will be splitted into words. 
* The model will be <b>provided one word and the photo and generate the next word</b>.
* Then the <b>first two words of the description</b> will be provided to the model as <b>input with the image to generate the next word</b>. This is how the model will be trained. 
* So we will have two features, `x1` (image) , `x2` (text_sequence) and one target variable, `y`.
"""

def prepare_data(image_keys):
    """
    """

    # x1  will store the image feature
    # x2  will store one sequence
    # y   will store the next sequence
    x1, x2, y = [], [], []

    # Iterate through all the images 
    for image in image_keys:

        # Store the caption of that image took of train_validate_image_caption
        caption = train_validate_image_caption[image]

        # Split the image into tokens
        seq = tokenizer.texts_to_sequences([caption])[0]

        length = len(seq)

        for i in range(1, length):

            x2_seq, y_seq = seq[:i] , seq[i]  

            # Pad the sequences
            # +-------------------------------------------------+
            # |             Output is a sparce matrix           |
            # |                                                 |
            # | * we can try other methods to avoid this format |
            # +-------------------------------------------------+
            x2_seq = pad_sequences([x2_seq], maxlen = max_len)[0]


            # Encode the output sequence                
            y_seq = to_categorical([y_seq], num_classes = vocab_len)[0]

            x1.append( train_validate_features[image.split(".")[0]][0] )

            x2.append(x2_seq)

            y.append(y_seq)
                
    return x1, x2, y

# Function that store a nested list of floats into a filename using a mode
def store_prepare_data(filename, mode, data):
  """
  Store :   data list(list(float32)) 
  Into  :   filename 
  Using :   mode ('w', 'a')
  """

  try:
    with open(filename, mode) as file:
      for list1 in data:
        for i in range(0, len(list1)):
          file.write(str(list1[i]))
          if i != (len(list1) -1):
            file.write(",")
        file.write("\n") 
    return True
  except:
    return False

# Number of samples handled at the same time, it must be a divider of the number of data de select
BATCH_SIZE_TRAIN = 1750
BATCH_SIZE_VAL   = 375
x1_train, x2_train, y_train = [], [], []

# Montoring the memory consume
monitoring_cumulated_memory_size = pd.DataFrame(
    columns=["number_samples", 
             "memory_size_image_features", 
             "memory_size_sequences", 
             "memory_size_next_sequence"]
             )
fn_train_x1 = FLICKR_PATH_DIR_8K_5C_MODEL + 'train_x1_other' + '.txt'
fn_train_x2 = FLICKR_PATH_DIR_8K_5C_MODEL + 'train_x2_other' + '.txt'
fn_train_y = FLICKR_PATH_DIR_8K_5C_MODEL + 'train_y_other' + '.txt'

fn_validate_x1 = FLICKR_PATH_DIR_8K_5C_MODEL + 'validate_x1_other' + '.txt'
fn_validate_x2 = FLICKR_PATH_DIR_8K_5C_MODEL + 'validate_x2_other' + '.txt'
fn_validate_y = FLICKR_PATH_DIR_8K_5C_MODEL + 'validate_y_other' + '.txt'

"""<u>Warning</u> :
* Since the training and validation variables have already been computed and stored, <b>runing the following cell is useless !</b>
* The `if False:` is there just in case someone run this cell without reading the documentation.    
"""

# if False :
print("Computing and processing training data ...")
for  i in range(0, len(train_validate_images_processed[0:28000]), BATCH_SIZE_TRAIN):
  
  train_x1,     train_x2,     train_y     = prepare_data( train_validate_images_processed[i:i+BATCH_SIZE_TRAIN] )

  # +-------------------------------------------------+
  # |       UNCOMMENT TO STORE AGAIN THE DATA         |    
  # +-------------------------------------------------+
  # | 1- Change the file names to not overwrite them  |
  # | 2- Run the cell                                 |
  # +-------------------------------------------------+
  print("Storring data {} ...".format(i))
  store_prepare_data(fn_train_x1, 'a', train_x1)
  store_prepare_data(fn_train_x2, 'a', train_x2)
  store_prepare_data(fn_train_y, 'a', train_y)


  # monitoring_cumulated_memory_size = monitoring_cumulated_memory_size.append(
  #    {"number_samples": i+BATCH_SIZE, 
  #     "memory_size_image_features":  getsizeof(x1_train),
  #     "memory_size_sequences":       getsizeof(x2_train),
  #     "memory_size_next_sequence":   getsizeof(y_train)
  #     }, ignore_index=True)

del train_x1
del train_x2 
del train_y

print("Computing and processing validation data ...")
for  i in range(0, len(train_validate_images_processed[28000:28000+6000]), BATCH_SIZE_VAL):
  
  validate_x1,  validate_x2,  validate_y    = prepare_data( train_validate_images_processed[i:i+BATCH_SIZE_VAL] )

  # +-------------------------------------------------+
  # |       UNCOMMENT TO STORE AGAIN THE DATA         |    
  # +-------------------------------------------------+
  # | 1- Change the file names to not overwrite them  |
  # | 2- Run the cell                                 |
  # +-------------------------------------------------+
  print("Storring data {} ...".format(i))
  store_prepare_data(fn_validate_x1, 'a', validate_x1)
  store_prepare_data(fn_validate_x2, 'a', validate_x2)
  store_prepare_data(fn_validate_y, 'a', validate_y)



"""## Sentence generation model

### Model architecture
This section presents the model of sentence generation. It uses block of <b>Keras layers</b> with the following specificities:

* Feature extractor block :

  * `Input layer` : This as an input shape of `(4096,)` because it's the shape of the output predictions of VGG16.

  * `Dropout layer` : This is a regularizer technique that <b>reduces the odds of overfitting</b> by dropping out neurons at random, during every epoch (or, when using a minibatch approach, during every minibatch). </br>
  <b>Dropping out neurons</b> happens by attaching Bernoulli variables to the neural outputs (Srivastava et al., 2014). These variables, which take the value of 1 with probability p and 0 with 1−p, <b>help reduce overfitting by “making the presence of other (..) units unreliable”</b>. </br>

 
  * `Dense layer` : Let's see what this will look like.
    These types of layers are fully connected or dense layers. So when we use a dense layer in keras , we're simply stating that the neurons in that layer are fully connected to the neurons in the previous layer.

    * `Input layer` : This as an input shape of `(max_len,)` corresponding to the length of the maximum sentence.

    * `Embedding layer` : A word embedding is a class of approaches for representing words and documents using a dense vector representation.. It requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.

      The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.

      It is a flexible layer that can be used in a variety of ways, such as:

      * It can be used alone to learn a word embedding that can be saved and used in another model later.
      * It can be used as part of a deep learning model where the embedding is learned along with the model itself.
      * It can be used to load a pre-trained word embedding model, a type of transfer learning.
    
    * `LSTM layer` : An LSTM layer above provides a sequence output rather than a single value output to the LSTM layer below. Specifically, one output per input time step, rather than one output time step for all input time steps.</br>
    ![image.png](data:image/png;base64,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)
"""

# Feature extractor model
input_1     = Input(shape=(4096,))
droplayer   = Dropout(0.5)(input_1)
denselayer  = Dense(256, activation='relu')(droplayer)

# Sequence model
input_2     = Input(shape=(max_len,))
embedding   = Embedding(vocab_len, 256, mask_zero=True)(input_2)
droplayer_  = Dropout(0.5)(embedding)
lstm        = LSTM(256)(droplayer_)

# Decoder model
decoder1    = add([denselayer, lstm])
decoder2    = Dense(256, activation='relu')(decoder1)
outputs     = Dense(vocab_len, activation='softmax')(decoder2)

# Optimizer 
optimizer   = Adam(learning_rate=0.001)

# Tie it together [image, seq] [word]
model = Model(inputs=[input_1, input_2], 
              outputs=outputs)

model.compile(loss      = 'categorical_crossentropy', 
              optimizer = optimizer,
              metrics   = ["accuracy"])

# Summarize model
print(model.summary())

fn_model = FLICKR_PATH_DIR_8K_5C_MODEL + "model.png"
plot_model(model, to_file=fn_model, show_shapes=True)

"""<u> Way to proceed</u> :

* As the trainning and validation data is memory consuming, having them into variables and thus in RAM is impossible and not reasonable. That's why we previously stored this data in 6 different files.
* We are going to be able to create a generator that act like an iterator and pass it to `model.fit()` methode. Therefore, it will be possible to load data per `BATCH_SIZE_LOAD` to not exceed the RAM memory capacity.
* The model with thus deal with a `BATCH_SIZE_LOAD` data size.
"""

def read_nested_array_from_file(filename):
  """
  String -> List(List(float32))
  """
  lr = []
  cpt = 0
  with open(filename, 'r') as file:
    for line in file:
      #list_line = line.split(",")
      #list_line[len(list_line)-1] = list_line[len(list_line)-1].rstrip('\n')
      #lr.append(line.split(","))
      cpt += 1

  #return lr
  return cpt

tx1 = read_nested_array_from_file(fn_train_x1)
print(tx1)
tx2 = read_nested_array_from_file(fn_train_x2)
print(tx2)
ty = read_nested_array_from_file(fn_train_y)
print(ty)

tx1 = read_nested_array_from_file(fn_validate_x1)
print(tx1)
tx2 = read_nested_array_from_file(fn_validate_x2)
print(tx2)
ty = read_nested_array_from_file(fn_validate_y)
print(ty)



"""Note :
We assume taking 1 comment per image.
* The 3 following <b>training</b> files :
  * `train_x1_other.txt` in `fn_train_x1` variable  
  * `train_x2_other.txt` in `fn_train_x2` variable
  * `train_y_other.txt` in `fn_train_y` variable

  ... contain <b>365601</b> lines. This is a difficult number wcause wie have to use a `BATCH_SIZE` between for example `[[60;600]]` and this number could only give us a `BATCH_SIZE = 3` which is too small. Therefore, we will <b>not consider the laster element</b> and thus have <b>365600</b> elements. 
  ```bash
  # Do this for the 3 files of training
  head -n -1 NAME_OF_TRAINING_FILE.txt > TMP_NAME_TRAINING_FILE.txt
  mv TMP_NAME_TRAINIG_FILE.txt NAME_OF_TRAINING_FILE.txt

  # Verify the number lines
  wc -n -1 NAME_OF_TRAINING_FILE.txt
  ```
  Thus, we will use a <b>`BATCH_SIZE_LOAD_TRAINING = 1828`</b>

* The 3 following <b>validation</b> files :
  * `validate_x1_other.txt` in `fn_validate_x1` variable  
  * `validate_x2_other.txt` in `fn_validate_x2` variable
  * `validate_y_other.txt` in `fn_validate_y` variable

  ... contain <b>79332</b> lines. Thus, we will use a <b>`BATCH_SIZE_LOAD_VALIDATION = 1202`</b>
"""

checkpoint_path = FLICKR_PATH_DIR_8K_5C_MODEL_CHECKPOINTS_DIR + "model-ep04-loss3.866-val_loss3.746.ckpt"

# Since my model has already been trained, we lead the weight using the checkpoints
model.load_weights(checkpoint_path)

model.history

BATCH_SIZE_LOAD_TRAINING = 1828
BATCH_SIZE_LOAD_VALIDATION = 1202

# Generator that takes file paths of x1, x2 and y and a batch_size and yields the output data corresponding to the input training of the model
def generate_nested_array_from_file(path_x1, path_x2, path_y, batch_size):
  """
  String * String * String * int -> Array(np.array(np.array(float32)) * np.array(float32))
  Hypothese : the 3 files constains the same number of lines.
  """
  inputs_1 = []
  inputs_2  = []
  targets = []
  batch_count = 0

  while True:
    with open(path_x1, 'r') as f_x1, open(path_x2, 'r') as f_x2, open(path_y, 'r') as f_y:

          # Iterate over the 3 files to extract the lists in each line
          for line_f_x1, line_f_x2, line_f_y in zip(f_x1, f_x2, f_y):

            # Process x1
            x1            = line_f_x1.split(",")
            x1[len(x1)-1] = x1[len(x1)-1].rstrip('\n')

            # Process x2
            x2            = line_f_x2.split(",")
            x2[len(x2)-1] = x2[len(x2)-1].rstrip('\n')

            # Process y
            y           = line_f_y.split(",")
            y[len(y)-1] = y[len(y)-1].rstrip('\n')

            # Agregate all the lists
            inputs_1.append(x1)
            inputs_2.append(x2)
            targets.append(y)

            batch_count += 1

            if batch_count > batch_size:
              X1  = np.array(inputs_1, dtype='float32')
              X2  = np.array(inputs_2, dtype='float32')
              Y   = np.array(targets, dtype='float32')
              yield ([X1, X2], Y)
              inputs_1  = []
              inputs_2  = []
              targets   = []
              batch_count = 0


# Define checkpoint callback
fn_model_sentence_generation = FLICKR_PATH_DIR_8K_5C_MODEL_CHECKPOINTS_DIR + 'model-ep{epoch:02d}-loss{loss:.3f}-val_loss{val_loss:.3f}.ckpt'

model_checkpoint_callback = ModelCheckpoint(filepath          = fn_model_sentence_generation, 
                                            monitor           = 'val_accuracy', 
                                            mode              = 'max',
                                            save_best_only    = True,
                                            save_weights_only = True) 


# Fit model
# Model weights are saved at the end of every epoch, if it's the best seen so far.
history = model.fit(generate_nested_array_from_file(fn_train_x1, fn_train_x2, fn_train_y, BATCH_SIZE_LOAD_TRAINING),   
                    steps_per_epoch   = 409335 / BATCH_SIZE_LOAD_TRAINING,           
                    verbose           = 1,            
                    epochs            = 20,            
                    callbacks         = [model_checkpoint_callback], 
                    validation_data   = generate_nested_array_from_file(fn_validate_x1, fn_validate_x2, fn_validate_y, BATCH_SIZE_LOAD_VALIDATION),
                    validation_steps  = 88309 / BATCH_SIZE_LOAD_VALIDATION)

"""```python
Epoch 1/50
2823/2823 [==============================] - 2537s 897ms/step - loss: 4.8933 - accuracy: 0.2354 - val_loss: 4.3545 - val_accuracy: 0.2685
Epoch 2/50
2823/2823 [==============================] - 2670s 946ms/step - loss: 4.1519 - accuracy: 0.2758 - val_loss: 4.0638 - val_accuracy: 0.2825
Epoch 3/50
2823/2823 [==============================] - 2737s 970ms/step - loss: 4.0394 - accuracy: 0.2814 - val_loss: 3.9968 - val_accuracy: 0.2888
Epoch 4/50
2823/2823 [==============================] - 2764s 979ms/step - loss: 3.9954 - accuracy: 0.2842 - val_loss: 3.9407 - val_accuracy: 0.2917
Epoch 5/50
2823/2823 [==============================] - 2707s 959ms/step - loss: 3.9675 - accuracy: 0.2847 - val_loss: 3.9001 - val_accuracy: 0.2915
Epoch 6/50
2823/2823 [==============================] - 2667s 945ms/step - loss: 3.9617 - accuracy: 0.2853 - val_loss: 3.9085 - val_accuracy: 0.2890
Epoch 7/50
2823/2823 [==============================] - 2813s 997ms/step - loss: 3.9562 - accuracy: 0.2852 - val_loss: 3.8850 - val_accuracy: 0.2896
Epoch 8/50
2823/2823 [==============================] - 2642s 936ms/step - loss: 3.9470 - accuracy: 0.2856 - val_loss: 3.8978 - val_accuracy: 0.2907
Epoch 9/50
2823/2823 [==============================] - 2650s 939ms/step - loss: 3.9469 - accuracy: 0.2862 - val_loss: 3.8888 - val_accuracy: 0.2900
Epoch 10/50
2823/2823 [==============================] - 2652s 940ms/step - loss: 3.9477 - accuracy: 0.2864 - val_loss: 3.8735 - val_accuracy: 0.2906
Epoch 11/50
2823/2823 [==============================] - 2865s 1s/step - loss: 3.9525 - accuracy: 0.2861 - val_loss: 3.8665 - val_accuracy: 0.2907
Epoch 12/50
2823/2823 [==============================] - 3077s 1s/step - loss: 3.9584 - accuracy: 0.2847 - val_loss: 3.8746 - val_accuracy: 0.2919
Epoch 13/50
2823/2823 [==============================] - 2768s 981ms/step - loss: 3.9665 - accuracy: 0.2845 - val_loss: 3.8852 - val_accuracy: 0.2897
Epoch 14/50
2823/2823 [==============================] - 2635s 933ms/step - loss: 3.9744 - accuracy: 0.2833 - val_loss: 3.8606 - val_accuracy: 0.2898
Epoch 15/50
 125/2823 [>.............................] - ETA: 34:11 - loss: 3.9799 - accuracy: 0.2848
 ```

Console log of the model training :
```python
Epoch 1/20
136445/136445 [==============================] - 34069s 250ms/step - loss: 4.9246 - accuracy: 0.2636 - val_loss: 4.4610 - val_accuracy: 0.3125
Epoch 2/20
 47295/136445 [=========>....................] - ETA: 6:08:48 - loss: 4.6445 - accuracy: 0.3090
 Epoch 1/20
223/223 [==============================] - 1536s 7s/step - loss: 3.2912 - accuracy: 0.3557 - val_loss: 3.1625 - val_accuracy: 0.3679
Epoch 2/20
223/223 [==============================] - 1554s 7s/step - loss: 3.2729 - accuracy: 0.3575 - val_loss: 3.1215 - val_accuracy: 0.3695
Epoch 3/20
223/223 [==============================] - 1571s 7s/step - loss: 3.2546 - accuracy: 0.3592 - val_loss: 3.1185 - val_accuracy: 0.3720
Epoch 4/20
223/223 [==============================] - 1563s 7s/step - loss: 3.2446 - accuracy: 0.3599 - val_loss: 3.1377 - val_accuracy: 0.3708
Epoch 5/20
223/223 [==============================] - 1554s 7s/step - loss: 3.2293 - accuracy: 0.3623 - val_loss: 3.1170 - val_accuracy: 0.3716
Epoch 6/20
223/223 [==============================] - 1545s 7s/step - loss: 3.2073 - accuracy: 0.3631 - val_loss: 3.1154 - val_accuracy: 0.3724
Epoch 7/20
223/223 [==============================] - 1547s 7s/step - loss: 3.1912 - accuracy: 0.3650 - val_loss: 3.1065 - val_accuracy: 0.3738
Epoch 8/20
 85/223 [==========>...................] - ETA: 13:09 - loss: 3.1512 - accuracy: 0.3687
 ```
"""